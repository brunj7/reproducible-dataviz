---
title: "data processing"
author: "Julien Brun"
date: "9/12/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Summary

This vignette aims to showcase a use case using the 2 main functions of `metajam` - `download_d1_data` and `read_d1_files` using a data processing workflow developed by the NCO synthesis working group [Stream Elemental Cycling](https://lternet.edu/working-groups/global-patterns-stream-energy-nutrient-cycling/). 

The datasets used are from the [LTER site - Luquillo](http://luq.lter.network) and can be found in the PASTA data repository <http://dx.doi.org/doi:10.6073/pasta/f9df56348f510da0113b1e6012fa2967>. This data package is a collection of 8 datasets of stream water samples from 8 different locations of the Luquillo Mountains. 

Our **goal** is to read the data for the 8 different sampling sites and aggregate them into one harmonized dataset. We will use the metadata to check if the data structures and units are the same across the 8 different sampling sites before performing the aggregation.


## Libraries

```{r libraries, message=FALSE}
# For data download
# devtools::install_github("NCEAS/metajam")
library(metajam)  

# For wrangling the data
library(tidyverse)
library(lubridate)
```


## Download the datasets

```{r download, eval=FALSE}
# Where we download the data from DataONE on your local machine
data_folder <- "Stream_data_LUQ"

# Create the local directory to store datasets
dir.create(data_folder, showWarnings = FALSE)

# Get the datasets unique identifiers
test_datasets_listing <- read_csv("./data/LTER-LUQ_DatasetsListing_SearchedData.csv")

# Keep only the LUQ related datasets
luq_test_datasets <- test_datasets_listing %>%
  select(-description, -doi) %>%
  na.omit() %>%
  arrange(filename) # sort the data sets alphabetically

## Batch download the datasets
local_datasets <- map(luq_test_datasets$dataset_URL, ~download_d1_data(.x, data_folder))
```
At this point, you should have all the data and the metadata downloaded inside your main directory; `Stream_data_LUQ` in this example. `metajam` organize the files as follow: 
- Each dataset is stored a sub-directory named after the package DOI and the file name
- Inside this sub-directory, you will find
   - the data: `my_data.csv`
   - the raw EML with the naming convention _file name_ + `__full_metadata.xml`: `my_data__full_metadata.xml`
   - the package level metadata summary with the naming convention _file name_ + `__summary_metadata.csv`: `my_data__summary_metadata.csv`
   - If relevant, the attribute level metadata with the naming convention _file name_ + `__attribute_metadata.csv`: `my_data__attribute_metadata.csv`
   - If relevant, the factor level metadata with the naming convention _file name_ + `__attribute_factor_metadata.csv`: my_data`__attribute_factor_metadata.csv`



## Let us map the sampled streams 
```{r}
library(leaflet)
library(leaflet.extras)  # handle KML format

# Crea
leaflet() %>% setView(-65.82, 18.32, 14) %>%
       addTiles() %>%
       addKML(kml, labelProperty = 'name', popupProperty = 'description')
```


## Read the data and metadata in your R environment

```{r read_data, eval=FALSE}
# You could list the datasets dowloaded in the `Data_SEC` folder 
# local_datasets <- dir(data_folder, full.names = TRUE)

# or you can directly use the outputed paths from download_d1_data 
# Read all the datasets and their associated metadata in as a named list
luq_datasets <- map(local_datasets, read_d1_files) %>% 
  set_names(map(., ~.x$summary_metadata$value[.x$summary_metadata$name == "File_Name"]))
```
## Perform checks on data structure

```{r attributes, eval=FALSE}
# list all the attributes
attributes_luq <- luq_datasets %>% map("data") %>% map(colnames)

# Check if they are identical by comparing all against the first site
for(ds in names(attributes_luq)) {
  print(identical(attributes_luq[[1]], attributes_luq[[ds]]))
}

#> => We are good, same data structure across the sampling sites
```

## Perform checks on the units

```{r units, eval=FALSE}
# List all the units used
luq_units <- luq_datasets %>% map("attribute_metadata") %>% map(~.[["unit"]])

# Check if they are identical by comparing all against the first site
for(us in names(luq_units)) {
  print(identical(luq_units[[1]], luq_units[[us]]))
}
```


## Append all the sampling sites into one master dataset

```{r combine, eval=FALSE}
# bind the sampling sites data into one master dataset for LUQ
all_sites_luq <- luq_datasets %>%
  map("data") %>% 
  bind_rows(.id = "prov")

# Replace -9999 with NAs
all_sites_luq[all_sites_luq == -9999] <- NA

# fix the dates
all_sites_luq$Sample_Date <- mdy(gsub(" 0:00", "", all_sites_luq$Sample_Date)) ## Cuenca2 as 0:00 appended to the date

# Write as csv
# write_csv(all_sites_luq, "stream_chem_all_LUQ.csv")
```


```{r}
library(ggplot2)

ggplot(all_sites_luq) + 
    geom_line(aes(Sample_Date, pH, color=`Sample_ID`))
```